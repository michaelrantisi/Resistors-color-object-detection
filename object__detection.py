# -*- coding: utf-8 -*-
"""object _detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c9rFEF6krqxuL2ci86JAAs_zG58276SK
"""

import matplotlib.pyplot as plt
import numpy as np
import PIL
import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
import pathlib
import pandas as pd
import seaborn as sns

data_dir = pathlib.Path('/content/drive/MyDrive/Image data').with_suffix('')

data = '/content/drive/MyDrive/Image data'

paths = [path.parts[-2:] for path in
         pathlib.Path(data).rglob('*.*')]
df = pd.DataFrame(data=paths, columns=['Class','Images'])
df = df.sort_values('Class',ascending=True)
df.reset_index(drop=True, inplace=True)
df

print('Count the number of image datasets')
print("Image Count : {}".format(len(df.Images)))
print("Class Count : {} \n".format(len(df['Class'].value_counts())))
print('Count the number of images in each class')
print(df['Class'].value_counts())

fig, ax = plt.subplots(figsize=(15,9))
sns.countplot(data=df, x='Class')
plt.title('Graph of the count of each class on the padang cuisine image dataset')
plt.xlabel('\n Image Class')
plt.ylabel('Count Image')
ax.set_xticklabels(ax.get_xticklabels(), rotation=90)

plt.show()

image_count = len(list(data_dir.glob('*/*.jpeg')))+len(list(data_dir.glob('*/*.jpg')))+len(list(data_dir.glob('*/*.png')))
print(image_count)

directories = {'1.5k': '/content/drive/MyDrive/Image data/1.5k Ohm resistor',
               '2.7k': '/content/drive/MyDrive/Image data/2.7k Ohm resistor',
               '10k': '/content/drive/MyDrive/Image data/10k Ohm resistor',
               '100k': '/content/drive/MyDrive/Image data/100k ohm resistor',
               '22k': '/content/drive/MyDrive/Image data/22k Ohm resistor',
               '3.3k': '/content/drive/MyDrive/Image data/3.3k ohm only',
               '33': '/content/drive/MyDrive/Image data/33 ohm resistor',
               '330': '/content/drive/MyDrive/Image data/330 Ohm resistor',
               '4.6k': '/content/drive/MyDrive/Image data/4.6k Ohm resistor',
               '47': '/content/drive/MyDrive/Image data/47 Ohm resistor',
               '470': '/content/drive/MyDrive/Image data/470 Ohm resistor',
               '5.6k': '/content/drive/MyDrive/Image data/5.6k Ohm resistor',
               '10': '/content/drive/MyDrive/Image data/10 Ohm resistor',
               '120': '/content/drive/MyDrive/Image data/120 Ohm resistor',
               '1k': '/content/drive/MyDrive/Image data/1k ohm only',

              }

def get_dims(file):
  im = Image.open(file)

  arr = np.array(im)
  h,w,d = arr.shape
  return h,w

def get_rgb(file):
    im = Image.open(file)
    arr = np.array(im)
    r_mean = np.mean(arr[:,:,0])
    g_mean = np.mean(arr[:,:,1])
    b_mean = np.mean(arr[:,:,2])
    bright_mean = np.mean([r_mean, g_mean, b_mean])
    return r_mean, g_mean, b_mean, bright_mean

import os
import dask.bag as db
import dask.array as da
from dask.diagnostics import ProgressBar
from PIL import Image
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt



for n,d in directories.items():
  filepath = d
  filelist = [os.path.join(filepath, f) for f in os.listdir(filepath)]
  dims = db.from_sequence(filelist).map(get_dims)

  with ProgressBar():
    dims = dims.compute()
    dim_df = pd.DataFrame(dims, columns=['height', 'width'])
    sizes = dim_df.groupby(['height', 'width']).size().reset_index().rename(columns={0:'count'})
    sizes.plot.scatter(x='width', y='height');
    plt.title('Image Sizes (pixels) | {}'.format(n))

filelist = []
for n, d in directories.items():
    filepath = d
    filelist += [os.path.join(filepath, f) for f in os.listdir(filepath)]

r_array=[]
g_array=[]
b_array=[]
bright_array=[]
for file in filelist:
    rm, gm, bm, brm = get_rgb(file)
    r_array.append(rm)
    g_array.append(gm)
    b_array.append(bm)
    bright_array.append(brm)


rgbb_df=pd.DataFrame({ 'R':r_array,
                       'G':g_array,
                       'B':b_array,
                       'Brightness':bright_array})
fig1 = plt.figure()
sns.histplot(data=rgbb_df, x = 'R',  bins=30)
fig2 = plt.figure()
sns.histplot(data=rgbb_df, x = 'G',  bins=30)
fig3=plt.figure()
sns.histplot(data=rgbb_df, x = 'B',  bins=30)
fig4=plt.figure()
sns.histplot(data=rgbb_df, x = 'Brightness',  bins=30)

rgbb_df.describe()

rgbb_df.skew()

rgbb_df.kurtosis()

from PIL import Image
import os

for n, d in directories.items():
    filepath = d
    filelist = [os.path.join(filepath, f) for f in os.listdir(filepath)]
    for file in filelist:
        im = Image.open(file)
        im = im.resize((128,128))
        im = im.convert('RGB')
        im.save(file)

data_dir_ini = pathlib.Path('data_ini').with_suffix('')
data_ini = '/content/drive/MyDrive/Image data'

paths_ini = [path.parts[-2:] for path in
         pathlib.Path(data_ini).rglob('*.*')]
df_ini = pd.DataFrame(data=paths_ini, columns=['Class','Images'])
df_ini = df_ini.sort_values('Class',ascending=True)
df_ini.reset_index(drop=True, inplace=True)
df_ini

print('Count the number of image datasets')
print("Image Count : {}".format(len(df_ini.Images)))
print("Class Count : {} \n".format(len(df_ini['Class'].value_counts())))
print('Count the number of images in each class')
print(df['Class'].value_counts())
image_count = len(list(data_dir.glob('*/*.jpeg')))+len(list(data_dir.glob('*/*.jpg')))+len(list(data_dir.glob('*/*.png'))) # folder/name <- */*
print(image_count)

directories = {'1.5k': '/content/drive/MyDrive/Image data/1.5k Ohm resistor',
               '2.7k': '/content/drive/MyDrive/Image data/2.7k Ohm resistor',
               '10k': '/content/drive/MyDrive/Image data/10k Ohm resistor',
               '100k': '/content/drive/MyDrive/Image data/100k ohm resistor',
               '22k': '/content/drive/MyDrive/Image data/22k Ohm resistor',
               '3.3k': '/content/drive/MyDrive/Image data/3.3k ohm only',
               '33': '/content/drive/MyDrive/Image data/33 ohm resistor',
               '330': '/content/drive/MyDrive/Image data/330 Ohm resistor',
               '4.6k': '/content/drive/MyDrive/Image data/4.6k Ohm resistor',
               '47': '/content/drive/MyDrive/Image data/47 Ohm resistor',
               '470': '/content/drive/MyDrive/Image data/470 Ohm resistor',
               '5.6k': '/content/drive/MyDrive/Image data/5.6k Ohm resistor',
               '10': '/content/drive/MyDrive/Image data/10 Ohm resistor',
               '120': '/content/drive/MyDrive/Image data/120 Ohm resistor',
               '1k': '/content/drive/MyDrive/Image data/1k ohm only',

              }
def get_dims(file):
  im = Image.open(file)

  arr = np.array(im)
  h,w,d = arr.shape
  return h,w

def get_rgb(file):
    im = Image.open(file)
    arr = np.array(im)
    r_mean = np.mean(arr[:,:,0])
    g_mean = np.mean(arr[:,:,1])
    b_mean = np.mean(arr[:,:,2])
    bright_mean = np.mean([r_mean, g_mean, b_mean])
    return r_mean, g_mean, b_mean, bright_mean

for n,d in directories.items():
  filepath = d
  filelist = [os.path.join(filepath, f) for f in os.listdir(filepath)]
  dims = db.from_sequence(filelist).map(get_dims)

  with ProgressBar():
    dims = dims.compute()
    dim_df = pd.DataFrame(dims, columns=['height', 'width'])
    sizes = dim_df.groupby(['height', 'width']).size().reset_index().rename(columns={0:'count'})
    sizes.plot.scatter(x='width', y='height');
    plt.title('Image Sizes (pixels) | {}'.format(n))

batch_size = 16
img_height = 128
img_width = 128

train_ds = tf.keras.utils.image_dataset_from_directory(
  data_dir,
  validation_split=0.2,
  subset="training",
  seed=123,
  image_size=(img_height, img_width),
  batch_size=batch_size)

val_ds = tf.keras.utils.image_dataset_from_directory(  #the tf terminology for this method demands the term validation where we we might otherwise use the term test. Just know that validation in this case means test, we're not doing a 3-way split of the data nor k-fold cross-validation
  data_dir,
  validation_split=0.2,
  subset="validation",
  seed=123,
  image_size=(img_height, img_width),
  batch_size=batch_size)

class_names = train_ds.class_names
print(class_names)

plt.figure(figsize=(10, 10))
for images, labels in train_ds.take(1):
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    plt.imshow(images[i].numpy().astype("uint8"))
    plt.title(class_names[labels[i]])
    plt.axis("off")

class_names_train = train_ds.class_names
print(class_names_train)

class_names_test = val_ds.class_names
print(class_names_test)

normalization_layer = layers.Rescaling(1./255)

num_classes = len(class_names_train)

model = Sequential([
  layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),
  layers.Conv2D(16, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Conv2D(32, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Conv2D(64, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Flatten(),
  layers.Dense(128, activation='relu'),
  layers.Dense(num_classes)
])

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
epochs=30
history_CNN = model.fit(
  train_ds,
  validation_data=val_ds,
  epochs=epochs
)

acc = history_CNN.history['accuracy']
val_acc = history_CNN.history['val_accuracy']

loss = history_CNN.history['loss']
val_loss = history_CNN.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

img_height = 128
img_width = 128
data_augmentation = keras.Sequential(
  [
    layers.RandomFlip("horizontal",
                      input_shape=(img_height,
                                  img_width,
                                  3)),
    layers.RandomRotation(0.1),
    layers.RandomZoom(0.1),
  ]
)

plt.figure(figsize=(10, 10))
for images, _ in train_ds.take(1):
  for i in range(9):
    augmented_images = data_augmentation(images)
    ax = plt.subplot(3, 3, i + 1)
    plt.imshow(augmented_images[0].numpy().astype("uint8"))
    plt.axis("off")

from keras.optimizers import Adam
model2 = Sequential([
  data_augmentation,
  layers.Rescaling(1./255),
  layers.Conv2D(16, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Conv2D(32, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Conv2D(64, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Dropout(0.2),
  layers.Flatten(),
  layers.Dense(128, activation='relu'),
  layers.Dense(num_classes, name="outputs")
])
model2.compile(optimizer=Adam(learning_rate=0.001),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
model2.summary()

epochs = 100
history_augmentation = model2.fit(
  train_ds,
  validation_data=val_ds,
  epochs=epochs
)

acc = history_augmentation.history['accuracy']
val_acc = history_augmentation.history['val_accuracy']

loss = history_augmentation.history['loss']
val_loss = history_augmentation.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

model_alexnet2=keras.models.Sequential([
    data_augmentation,
    layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),
    keras.layers.Conv2D(filters=128, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(128,128,3)),
    keras.layers.BatchNormalization(),
    keras.layers.MaxPool2D(pool_size=(2,2)),
    keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding="same"),
    keras.layers.BatchNormalization(),
    keras.layers.MaxPool2D(pool_size=(3,3)),
    keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding="same"),
    keras.layers.BatchNormalization(),
    keras.layers.Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), activation='relu', padding="same"),
    keras.layers.BatchNormalization(),
    keras.layers.Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), activation='relu', padding="same"),
    keras.layers.BatchNormalization(),
    keras.layers.MaxPool2D(pool_size=(2,2)),
    keras.layers.Flatten(),
    keras.layers.Dense(1024,activation='relu'),
    keras.layers.Dropout(0.5),
    keras.layers.Dense(1024,activation='relu'),
    keras.layers.Dropout(0.5),
    keras.layers.Dense(15,activation='softmax')


])

model_alexnet2.compile(
    loss='sparse_categorical_crossentropy',
    optimizer=Adam(learning_rate=0.001),
    metrics=['accuracy']
)
model_alexnet2.summary()

history_alexnet2=model_alexnet2.fit(
    train_ds,
    epochs=200,
    validation_data=val_ds,
    validation_freq=1
)

epochs=200
acc = history_alexnet2.history['accuracy']
val_acc = history_alexnet2.history['val_accuracy']

loss = history_alexnet2.history['loss']
val_loss = history_alexnet2.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

import tensorflow as tf
from tensorflow.keras import layers, models, Sequential
from tensorflow.keras.applications import VGG16
from tensorflow.keras.optimizers import Adam
batch_size = 16
img_height = 224
img_width = 224

train_ds = tf.keras.utils.image_dataset_from_directory(
    data_dir,
    validation_split=0.2,
    subset="training",
    seed=123,
    image_size=(img_height, img_width),
    batch_size=batch_size)

val_ds = tf.keras.utils.image_dataset_from_directory(
    data_dir,
    validation_split=0.2,
    subset="validation",
    seed=123,
    image_size=(img_height, img_width),
    batch_size=batch_size)

def create_model(input_shape, n_classes, optimizer, fine_tune=0):
    base_model = VGG16(input_shape=input_shape, include_top=False, weights='imagenet')
    base_model.trainable = True  # Set the entire model as trainable

    if fine_tune > 0:
        for layer in base_model.layers[:-fine_tune]:
            layer.trainable = False
    else:
        base_model.trainable = False

    model = Sequential([
        base_model,  # Pre-trained base
        layers.Flatten(),
        layers.Dense(4096, activation='relu'),
        layers.Dropout(0.5),
        layers.Dense(4096, activation='relu'),
        layers.Dropout(0.5),
        layers.Dense(n_classes, activation='softmax')
    ])

    model.compile(optimizer=optimizer,
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    return model

input_shape = (224, 224, 3)
optim_1 = Adam(learning_rate=0.0001)
n_classes = len(train_ds.class_names)

vgg_model = create_model(input_shape, n_classes, optim_1, fine_tune=2)
vgg_model.summary()

epochs = 15


history = vgg_model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=epochs
)

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs_range = range(epochs)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

def get_final_accuracy(history):
    training_accuracy = history.history['accuracy'][-1]
    validation_accuracy = history.history['val_accuracy'][-1]
    return training_accuracy, validation_accuracy

accuracies = {
    'Basic CNN': get_final_accuracy(history_CNN),
    'CNN with Data Augmentation': get_final_accuracy(history_augmentation),
    'AlexNet-Inspired Model': get_final_accuracy(history_alexnet2),
    'VGG Model with Fine-Tuning': get_final_accuracy(history)
}


for model_name, (train_acc, val_acc) in accuracies.items():
    print(f"{model_name}: Training Accuracy = {train_acc:.4f}, Validation Accuracy = {val_acc:.4f}")

model.save('basic_cnn_model.h5')
model2.save('cnn_with_augmentation_model.h5')
model_alexnet2.save('alexnet_inspired_model.h5')
vgg_model.save('vgg_finetuned_model.h5')

from tensorflow.keras.models import load_model

loaded_basic_cnn_model = load_model('basic_cnn_model.h5')
loaded_cnn_with_augmentation_model = load_model('cnn_with_augmentation_model.h5')
loaded_alexnet_model = load_model('alexnet_inspired_model.h5')
loaded_vgg_model = load_model('vgg_finetuned_model.h5')